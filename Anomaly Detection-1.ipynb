{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e0c08-4a27-4266-80e1-365edb217acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1):-\n",
    "Anomaly detection, also known as outlier detection, is a machine learning and data analysis technique that aims to identify patterns or data points \n",
    "that deviate significantly from the expected or normal behavior within a dataset. The purpose of anomaly detection is to uncover unusual, rare, or \n",
    "abnormal instances that may indicate errors, anomalies, fraud, or other interesting and potentially important events. Here are key aspects of anomaly\n",
    "detection:\n",
    "\n",
    "Detecting Unusual Behavior: Anomaly detection focuses on finding data points or patterns that do not conform to the expected behavior of the majority\n",
    "of the data. These anomalies often represent unusual events, errors, or instances that require further investigation.\n",
    "\n",
    "Applications:\n",
    "\n",
    "Fraud Detection: In finance, anomaly detection is used to identify fraudulent transactions or activities that deviate from typical spending patterns.\n",
    "Network Security: In cybersecurity, anomaly detection helps detect unusual network behavior or intrusion attempts.\n",
    "Manufacturing: Anomaly detection is applied to identify defective products on a production line.\n",
    "Healthcare: In healthcare, it can be used to detect abnormal patient vitals or disease outbreaks.\n",
    "Quality Control: In manufacturing and industrial processes, it helps identify faulty equipment or processes.\n",
    "Methods:\n",
    "\n",
    "Statistical Methods: These methods rely on statistical models to capture the normal distribution of data and flag data points that fall outside \n",
    "predefined statistical boundaries.\n",
    "Machine Learning: Anomaly detection can also be performed using machine learning algorithms, such as isolation forests, one-class SVM, autoencoders, \n",
    "and k-nearest neighbors.\n",
    "Domain-Specific Rules: In some cases, domain-specific rules or heuristics are used to define anomalies based on expert knowledge.\n",
    "Unsupervised Learning: Anomaly detection is typically an unsupervised learning task, meaning it does not require labeled data indicating which\n",
    "instances are anomalies. Instead, it learns the normal behavior from the data itself.\n",
    "\n",
    "Thresholds and Scores: Anomaly detection algorithms often produce anomaly scores or probabilities for each data point. A threshold is set to\n",
    "determine which data points are considered anomalies based on these scores.\n",
    "\n",
    "Challenges: Anomaly detection can be challenging due to class imbalance (anomalies are often rare), the need for careful selection of thresholds,\n",
    "and the potential for false positives.\n",
    "\n",
    "Feedback Loop: In many applications, anomaly detection is not a one-time process but part of a continuous feedback loop. Detected anomalies may\n",
    "trigger further investigation or actions, and the model may need periodic retraining to adapt to changing data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d61d5-4b3c-4007-97ae-192bc8e57170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2):-\n",
    "Anomaly detection is a valuable technique, but it comes with several key challenges that practitioners and researchers must address to build effective\n",
    "anomaly detection systems. Here are some of the key challenges in anomaly detection:\n",
    "\n",
    "Imbalanced Data: Anomalies are often rare compared to normal instances. This class imbalance can lead to models that are biased toward the majority \n",
    "class, making it challenging to detect anomalies effectively.\n",
    "\n",
    "Labeling Anomalies: In many real-world scenarios, obtaining labeled data for anomalies can be difficult, expensive, or even impossible. Anomaly\n",
    "detection is often performed in an unsupervised or semi-supervised manner, which can make it challenging to evaluate and train models effectively.\n",
    "\n",
    "Changing Data Patterns: Anomalies can evolve over time, and the normal behavior in a dataset can change. An effective anomaly detection system must\n",
    "be able to adapt to shifting data patterns and detect new types of anomalies.\n",
    "\n",
    "Selection of Features: Choosing the right features or representations of data is crucial for successful anomaly detection. In high-dimensional spaces,\n",
    "feature selection or dimensionality reduction techniques may be necessary to avoid the curse of dimensionality.\n",
    "\n",
    "Model Selection: Selecting the most appropriate anomaly detection algorithm for a specific problem is challenging. Different algorithms have different\n",
    "strengths and weaknesses, and there is no one-size-fits-all solution.\n",
    "\n",
    "Threshold Selection: Determining an appropriate threshold for anomaly scores or probabilities can be difficult. Setting the threshold too high may \n",
    "result in missed anomalies (false negatives), while setting it too low may lead to an excessive number of false positives.\n",
    "\n",
    "Interpreting Results: Anomaly detection often produces anomaly scores or labels without providing clear explanations for why a particular instance is \n",
    "considered an anomaly. Understanding the reasons behind anomalies can be crucial for taking appropriate actions.\n",
    "\n",
    "Scalability: Anomaly detection algorithms must be scalable to handle large datasets efficiently. Some algorithms may struggle when faced with a high\n",
    "volume of data.\n",
    "\n",
    "Time Series Data: Anomalies in time series data may not only be isolated data points but also temporal patterns or sequences. Detecting such anomalies\n",
    "requires specialized techniques.\n",
    "\n",
    "Evaluation Metrics: Traditional classification metrics like accuracy may not be suitable for evaluating anomaly detection systems. Metrics like \n",
    "precision, recall, F1-score, and area under the ROC curve (AUC-ROC) are more relevant but need to be chosen based on the problem's characteristics.\n",
    "\n",
    "Concept Drift: In applications where data distributions change over time (concept drift), models may need to adapt to new normal and anomaly patterns. \n",
    "Continuous monitoring and retraining are essential.\n",
    "\n",
    "False Positives: Reducing false positives is crucial, especially in critical applications, to avoid unnecessary alarms or alerts that can lead to\n",
    "alert fatigue.\n",
    "\n",
    "Real-Time Processing: Some applications require real-time or near-real-time anomaly detection, which imposes additional computational and timing\n",
    "constraints.\n",
    "\n",
    "Domain Knowledge: Incorporating domain knowledge and expertise into the anomaly detection process can be challenging but is often necessary to define\n",
    "meaningful features, thresholds, and interpret results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c5f22-d75e-405f-9767-634c722a283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3):-\n",
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches to identifying anomalies within a dataset, and they \n",
    "differ in their underlying methodologies and the availability of labeled data. Here's how they differ:\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "\n",
    "Lack of Labels: Unsupervised anomaly detection is performed without the use of labeled data indicating which instances are anomalies. The algorithm\n",
    "must discover anomalies solely based on the data's intrinsic characteristics.\n",
    "\n",
    "Clustering or Density-Based: Unsupervised anomaly detection methods often rely on clustering or density-based techniques to identify anomalies.\n",
    "Examples of such methods include DBSCAN (Density-Based Spatial Clustering of Applications with Noise), isolation forests, and k-means clustering.\n",
    "\n",
    "Model Complexity: Unsupervised methods typically have fewer assumptions about the data and the nature of anomalies. They are generally more flexible\n",
    "in identifying anomalies of various types and shapes.\n",
    "\n",
    "Applications: Unsupervised anomaly detection is used when labeled data is scarce or costly to obtain. It is well-suited for scenarios where anomalies\n",
    "can take diverse and unexpected forms.\n",
    "\n",
    "Challenges: Challenges in unsupervised anomaly detection include setting appropriate thresholds for anomaly detection, dealing with class imbalance\n",
    "(as anomalies are often rare), and interpreting the detected anomalies without labeled examples.\n",
    "\n",
    "Supervised Anomaly Detection:\n",
    "\n",
    "Labeled Data: Supervised anomaly detection requires a labeled dataset where instances are categorized as either normal or anomalous. This labeled data\n",
    "is used to train a supervised machine learning model.\n",
    "\n",
    "Classification Algorithms: Supervised anomaly detection typically employs classification algorithms, such as logistic regression, support vector\n",
    "machines (SVM), decision trees, or deep learning models, to distinguish between normal and anomalous instances.\n",
    "\n",
    "Model Complexity: The choice of model complexity and the feature engineering process in supervised anomaly detection may be influenced by prior\n",
    "knowledge of the data and the nature of anomalies. These models can be more tailored to specific anomaly types.\n",
    "\n",
    "Applications: Supervised anomaly detection is used when labeled data is available and representative of the anomaly types of interest. It is effective\n",
    "for well-understood and relatively stable anomaly patterns.\n",
    "\n",
    "Challenges: Challenges in supervised anomaly detection include the need for a labeled dataset (which may be expensive or time-consuming to create), \n",
    "the risk of overfitting if the labeled data is limited, and the difficulty of adapting the model to changing anomaly patterns.\n",
    "\n",
    "Semi-Supervised Anomaly Detection:\n",
    "\n",
    "In some cases, a hybrid approach called semi-supervised anomaly detection is employed. In semi-supervised detection, a small amount of labeled data \n",
    "is used to build a supervised model, and the model is then applied to identify anomalies in the unlabeled data.\n",
    "\n",
    "The choice between unsupervised, supervised, or semi-supervised anomaly detection depends on the specific problem, the availability of labeled data, \n",
    "the complexity of the anomaly patterns, and the trade-offs between interpretability and flexibility. Unsupervised methods are useful when labeled data\n",
    "is scarce and anomaly patterns are diverse, while supervised methods are suitable when labeled data is abundant and well-representative of the \n",
    "anomalies of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab600a-7684-43e3-9aa6-b11ef053995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4):-\n",
    "Anomaly detection algorithms can be categorized into several main categories based on their underlying principles and techniques. These categories\n",
    "include:\n",
    "\n",
    "Statistical Methods:\n",
    "\n",
    "Z-Score (Standard Score): This method measures how many standard deviations a data point is from the mean. Data points that fall significantly outside\n",
    "a predefined range are considered anomalies.\n",
    "Percentile Score: Similar to Z-score, this method defines anomalies as data points that fall below or above a certain percentile threshold.\n",
    "\n",
    "Density-Based Methods:\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN identifies clusters of data points based on their density and classifies\n",
    "points that do not belong to any cluster as anomalies (noise).\n",
    "LOF (Local Outlier Factor): LOF measures the local density deviation of a data point with respect to its neighbors. Points with low LOF scores are\n",
    "considered anomalies.\n",
    "\n",
    "Distance-Based Methods:\n",
    "K-Nearest Neighbors (KNN): KNN identifies anomalies by comparing the distance of each data point to its k nearest neighbors. Outliers are those with \n",
    "significantly larger distances.\n",
    "Mahalanobis Distance: This metric measures the distance between a data point and the center of the distribution. Data points with high Mahalanobis \n",
    "distances are considered anomalies.\n",
    "\n",
    "Clustering Methods:\n",
    "K-Means Clustering: After clustering the data, points that do not belong to any cluster or belong to small, distant clusters may be considered \n",
    "anomalies.\n",
    "Hierarchical Clustering: Anomalies can be detected by examining the structure of the hierarchical clustering tree.\n",
    "Model-Based Methods:\n",
    "\n",
    "Gaussian Mixture Models (GMM): GMMs assume that data points follow a mixture of Gaussian distributions. Anomalies are data points with low likelihood\n",
    "under the fitted model.\n",
    "Autoencoders: Autoencoders are neural networks trained to reconstruct input data. Anomalies are identified as data points with high reconstruction \n",
    "errors.\n",
    "\n",
    "One-Class Classification:\n",
    "One-Class SVM (Support Vector Machine): This technique trains a model to separate the majority class from all other data points. Points on the \"other\"\n",
    "side of the decision boundary are considered anomalies.\n",
    "Isolation Forest: Isolation forests use decision trees to isolate anomalies more efficiently by exploiting their propensity to have shorter paths in \n",
    "the tree structure.\n",
    "\n",
    "Ensemble Methods:\n",
    "Random Forests: Random forests can be adapted for anomaly detection by measuring the outlierness of data points based on their behavior across\n",
    "multiple decision trees.\n",
    "Isolation Forest Ensembles: Combining multiple isolation forests can improve the detection of anomalies.\n",
    "\n",
    "Time Series Methods:\n",
    "SAX (Symbolic Aggregate Approximation): SAX represents time series data symbolically and detects anomalies by comparing symbol sequences.\n",
    "Prophet: Developed by Facebook, Prophet is a time series forecasting tool that can identify anomalies in time series data.\n",
    "\n",
    "Deep Learning Methods:\n",
    "Variational Autoencoders (VAEs): VAEs extend autoencoders to probabilistic modeling and can capture complex data distributions, making them useful \n",
    "for anomaly detection.\n",
    "Long Short-Term Memory (LSTM) Networks: LSTMs can model sequential data and detect anomalies by observing deviations from learned patterns.\n",
    "\n",
    "Graph-Based Methods:\n",
    "Graph-Based Anomaly Detection: These methods model data as a graph and identify anomalies based on the structure or connectivity of the graph.\n",
    "\n",
    "Domain-Specific Methods:\n",
    "Some domains, such as cybersecurity, finance, or healthcare, may have specialized anomaly detection techniques tailored to the specific \n",
    "characteristics and requirements of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc543962-ddfc-42fb-b3a7-e35c1b0acdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5):-\n",
    "Assumption of Normality:\n",
    "\n",
    "Assumption: Distance-based methods often assume that normal data points follow a certain distribution, typically a Gaussian (normal) distribution or a\n",
    "uniform distribution in feature space.\n",
    "\n",
    "Justification: By assuming normality, these methods can define a reference region around the mean or median of the data distribution. Data points\n",
    "that fall outside this region are considered anomalies.\n",
    "\n",
    "Local Neighborhood Density:\n",
    "Assumption: Distance-based methods assume that normal data points tend to be located in areas of higher data density, while anomalies are isolated or\n",
    "occur in regions of lower data density.\n",
    "\n",
    "Justification: By considering the local density of data points around each instance, these methods can identify points that have fewer neighbors or \n",
    "are farther away from their neighbors, suggesting that they may be anomalies.\n",
    "\n",
    "Threshold-Based Identification:\n",
    "Assumption: Distance-based methods establish a threshold or predefined distance limit beyond which data points are considered anomalies.\n",
    "Justification: This threshold is often set based on the assumed distribution of normal data points and is used to separate normal instances from\n",
    "anomalies. Points exceeding the threshold are flagged as anomalies.\n",
    "\n",
    "Distance Metric Choice:\n",
    "Assumption: Distance-based methods rely on a chosen distance metric (e.g., Euclidean distance, Mahalanobis distance) to quantify the similarity or \n",
    "dissimilarity between data points.\n",
    "\n",
    "Justification: The choice of distance metric is based on the assumption that it appropriately captures the underlying data distribution and the\n",
    "relationships between data points.\n",
    "\n",
    "Noisy Data Handling:\n",
    "Assumption: Distance-based methods assume that anomalies are associated with noise or errors in the data, and they aim to detect data points that\n",
    "deviate significantly from the expected patterns.\n",
    "\n",
    "Justification: Detecting noisy or erroneous data is essential for maintaining data quality and the accuracy of downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e81609-df9c-4a07-a88e-52883d41bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6):-\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores by assessing the local density deviation of each data point with respect to its\n",
    "neighbors. LOF is a density-based anomaly detection method that assigns a score to each data point, indicating how different its local density is \n",
    "from that of its neighbors. Higher LOF scores suggest that a data point is more likely to be an anomaly. Here's how LOF computes anomaly scores:\n",
    "\n",
    "Nearest Neighbors:\n",
    "\n",
    "LOF begins by defining a parameter k, which represents the number of nearest neighbors to consider when evaluating the local density of a data point.\n",
    "For each data point in the dataset, LOF identifies its k nearest neighbors based on a chosen distance metric (e.g., Euclidean distance).\n",
    "\n",
    "Local Reachability Density (LRD):\n",
    "The local reachability density of a data point, often denoted as LRD(x), is computed as the inverse of the average reachability distance of the data\n",
    "point to its k nearest neighbors. The reachability distance between two data points measures how \"reachable\" one point is from the other within their\n",
    "local neighborhood.\n",
    "LRD(x) for a data point x is calculated as follows:\n",
    "LRD(x) = 1 / (Σ reach_dist(x, y) for y in k-nearest neighbors of x)\n",
    "\n",
    "Local Density Ratio (LDR):\n",
    "To assess the local density of each data point, LOF computes the local density ratio (LDR) for the data point as the ratio of its LRD to the LRD \n",
    "of its neighbors. It quantifies how the local density of the data point compares to that of its neighbors.\n",
    "LDR(x) = LRD(x) / (Σ LRD(y) for y in k-nearest neighbors of x)\n",
    "\n",
    "Local Outlier Factor (LOF):\n",
    "The LOF for a data point x is defined as the average LDR of its k nearest neighbors relative to its own LDR. It quantifies how much the local density\n",
    "of the data point differs from the local densities of its neighbors. Higher LOF values indicate that the data point is an outlier.\n",
    "LOF(x) = (Σ LDR(y) for y in k-nearest neighbors of x) / (k * LDR(x))\n",
    "\n",
    "Anomaly Score:\n",
    "LOF assigns an anomaly score to each data point based on its LOF value. Data points with LOF values significantly greater than 1 are considered\n",
    "anomalies, as they exhibit a lower local density than their neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c436b4c-9248-441d-85e4-c22dd4ce1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7):-\n",
    "The Isolation Forest algorithm is a popular anomaly detection technique that operates based on the idea that anomalies are rare and can be isolated\n",
    "more quickly than normal instances in high-dimensional feature space. The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "n_estimators:\n",
    "This parameter specifies the number of base isolation trees to build. Increasing the number of trees can improve the algorithm's performance,\n",
    "but it also increases computational time. A larger number of trees can provide more reliable anomaly scores.\n",
    "\n",
    "max_samples:\n",
    "Max_samples controls the maximum number of data points sampled to create each isolation tree. It determines the size of the random subsets of the\n",
    "data used for building the individual trees. A smaller max_samples value can lead to faster tree construction but may result in less robust anomaly \n",
    "detection. Common values include \"auto\" (default), which sets max_samples to the size of the input data, or a specific integer or float value \n",
    "representing the desired number of samples.\n",
    "\n",
    "contamination:\n",
    "Contamination specifies the expected fraction of anomalies in the dataset. It is used to set the threshold for classifying instances as anomalies. \n",
    "For example, if contamination is set to 0.1, the algorithm will consider the top 10% of instances with the highest anomaly scores as anomalies.\n",
    "\n",
    "max_features:\n",
    "Max_features determines the maximum number of features to consider when splitting nodes in the isolation trees. Common values include \"auto\" \n",
    "(default), which uses all features, or an integer or float value representing the desired number of features. Limiting the number of features can \n",
    "help control overfitting and improve efficiency.\n",
    "\n",
    "bootstrap:\n",
    "If set to True (default), each isolation tree is built using bootstrapped samples from the dataset, which introduces randomness and diversity into \n",
    "the tree construction process. Setting it to False means that each tree will be built using the entire dataset.\n",
    "\n",
    "random_state:\n",
    "Random_state is a seed value that ensures reproducibility. By setting a specific random_state value, you can obtain consistent results across multiple\n",
    "runs.\n",
    "\n",
    "verbose:\n",
    "This parameter controls the verbosity of the algorithm's output during training. Setting it to 0 (default) means no output, while higher values \n",
    "provide more detailed progress information.\n",
    "\n",
    "n_jobs:\n",
    "N_jobs specifies the number of CPU cores to use for parallel execution during tree construction. Setting it to -1 uses all available CPU cores. This\n",
    "can significantly speed up the training process for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a021f2d-25f1-4f62-b664-3de81d4221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8):-\n",
    "In K-nearest neighbors (KNN) anomaly detection, the anomaly score of a data point is typically computed based on its distance to its K-nearest\n",
    "neighbors. The lower the distance, the less likely it is to be an anomaly. In your scenario, you have specified that a data point has only 2 neighbors\n",
    "of the same class within a radius of 0.5. However, you also mentioned that you are using K=10, which implies that you want to consider the 10 nearest\n",
    "neighbors.\n",
    "\n",
    "Given that you have only 2 neighbors within a radius of 0.5, you may not have enough neighbors to fill the K-nearest neighbors list (K=10). In such\n",
    "cases, you need to decide how to handle the situation. One common approach is to consider all available neighbors and assign a distance value to the \n",
    "remaining neighbors that are not within the specified radius.\n",
    "\n",
    "Let's break down the steps to compute the anomaly score:\n",
    "\n",
    "You have 2 neighbors within a radius of 0.5.\n",
    "\n",
    "Since K=10, you need to find the remaining 8 neighbors to reach a total of 10 neighbors. These additional neighbors should be within the specified \n",
    "radius of 0.5.\n",
    "\n",
    "Compute the distance between the data point and all 10 neighbors.\n",
    "\n",
    "Sort the distances in ascending order.\n",
    "\n",
    "The anomaly score can be computed based on the distances. Typically, it's calculated as a function of the distances, where lower distances indicate \n",
    "higher similarity (lower anomaly score), and higher distances indicate lower similarity (higher anomaly score).\n",
    "\n",
    "The specific formula for the anomaly score may vary depending on the variant of KNN anomaly detection being used, and it can be influenced by factors \n",
    "such as the distance metric and any weighting applied to the neighbors.\n",
    "\n",
    "Please note that without the actual data and distance values, it's not possible to provide an exact anomaly score in this hypothetical scenario. The\n",
    "anomaly score calculation would depend on the actual distances between the data point and its neighbors within the specified radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876199a2-822a-4ce1-aaeb-fc5ffbaf17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9):-\n",
    "In the Isolation Forest algorithm, the anomaly score for a data point is computed based on its average path length compared to the average path\n",
    "length of the trees in the forest. The average path length is a measure of how quickly a data point is isolated or separated from the rest of the \n",
    "data in the trees. Lower average path lengths indicate that a data point is isolated more quickly and is, therefore, more likely to be an anomaly.\n",
    "\n",
    "The formula to compute the anomaly score for a data point based on its average path length (APL) compared to the average APL of the trees is:\n",
    "\n",
    "Anomaly Score = 2^(-APL / c)\n",
    "\n",
    "where:\n",
    "APL is the average path length of the data point in the isolation trees.\n",
    "c is a constant that depends on the number of data points in the dataset. It's typically defined as:\n",
    "c(n) = 2 * (log(n - 1) + 0.5772156649) - (2 * (n - 1) / n)\n",
    "Given the information you provided:\n",
    "\n",
    "Number of trees (n_estimators) = 100\n",
    "Number of data points = 3000\n",
    "Average path length of the data point = 5.0\n",
    "First, you need to calculate the constant c:\n",
    "\n",
    "c(3000) = 2 * (log(3000 - 1) + 0.5772156649) - (2 * (3000 - 1) / 3000)\n",
    "\n",
    "Now, you can use the formula to compute the anomaly score:\n",
    "\n",
    "Anomaly Score = 2^(-5.0 / c(3000))\n",
    "\n",
    "Plug in the value of c(3000) to compute the anomaly score. This score will indicate the likelihood of the data point being an anomaly based on\n",
    "its average path length in the isolation forest. Lower scores suggest a higher likelihood of being an anomaly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
